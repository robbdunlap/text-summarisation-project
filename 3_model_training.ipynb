{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7322412d",
   "metadata": {},
   "source": [
    "# Part 3 - Training (aka *fine-tuning*) a Transformer model\n",
    "\n",
    "In this part we will finally train our very own Transformers model. We saw that the zero-shot model didn't produce great results, and that's probably because the model was trained on summarising news articles, not academic papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e04f77",
   "metadata": {},
   "source": [
    "These lines of code are typical setup for Sagemaker, we require them for training jobs: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74ce3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM role arn used for running training: arn:aws:iam::595714217589:role/service-role/AmazonSageMaker-ExecutionRole-20220331T161122\n",
      "S3 bucket used for storing artifacts: sagemaker-us-east-1-595714217589\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "\n",
    "# the \"get_execution_role()\" method doesn't work when running a notebook locally and using the API.\n",
    "# see the explanation in \"0b_data_prep_reviews_corrected.ipynb\" for an explanation and how to get \n",
    "# the proper variable\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = 'arn:aws:iam::595714217589:role/service-role/AmazonSageMaker-ExecutionRole-20220331T161122'\n",
    "\n",
    "print(f\"IAM role arn used for running training: {role}\")\n",
    "print(f\"S3 bucket used for storing artifacts: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2b90b",
   "metadata": {},
   "source": [
    "We are in the great position that we don't have to write our own training script. Instead we will use a script from the transformers library in Github: https://github.com/huggingface/transformers/blob/v4.6.1/examples/pytorch/summarization/run_summarization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0337381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd42fe8",
   "metadata": {},
   "source": [
    "These are the parameters for training, and this is one of the most important levers we can leverage once we are in the experimentation phase. Changing these parameters can influence the model performance and there will be a component of trial & error to find the best model. Also check out https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html for automated hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "429ed3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job - original version\n",
    "# hyperparameters={'per_device_train_batch_size': 4,\n",
    "#                  'per_device_eval_batch_size': 4,\n",
    "#                  'model_name_or_path': 'sshleifer/distilbart-cnn-12-6',\n",
    "#                  'train_file': '/opt/ml/input/data/datasets/train.csv',\n",
    "#                  'validation_file': '/opt/ml/input/data/datasets/val.csv',\n",
    "#                  'do_train': True,\n",
    "#                  'do_eval': True,\n",
    "#                  'do_predict': False,\n",
    "#                  'predict_with_generate': True,\n",
    "#                  'output_dir': '/opt/ml/model',\n",
    "#                  'num_train_epochs': 3,\n",
    "#                  'learning_rate': 5e-5,\n",
    "#                  'seed': 7,\n",
    "#                  'fp16': True,\n",
    "#                  'val_max_target_length': 20,\n",
    "#                  'text_column': 'text',\n",
    "#                  'summary_column': 'summary',\n",
    "#                  }\n",
    "\n",
    "#hyperparameters, which are passed into the training job - modified for my AWS account version\n",
    "hyperparameters={'per_device_train_batch_size': 4,\n",
    "                 'per_device_eval_batch_size': 4,\n",
    "                 'model_name_or_path': 'sshleifer/distilbart-cnn-12-6',\n",
    "                 'train_file': 's3://sagemaker-us-east-1-595714217589/summarization/data/train.csv',\n",
    "                 'validation_file': 's3://sagemaker-us-east-1-595714217589/summarization/data/val.csv',\n",
    "                 'do_train': True,\n",
    "                 'do_eval': True,\n",
    "                 'do_predict': False,\n",
    "                 'predict_with_generate': True,\n",
    "                 'output_dir': 's3://sagemaker-us-east-1-595714217589/summarization/output/model',\n",
    "                 'num_train_epochs': 3,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'fp16': True,\n",
    "                 'val_max_target_length': 20,\n",
    "                 'text_column': 'text',\n",
    "                 'summary_column': 'summary',\n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "086daaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_summarization.py',\n",
    "    source_dir='./examples/pytorch/summarization',\n",
    "    git_config=git_config,\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    instance_count=2,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution=distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150c672",
   "metadata": {},
   "source": [
    "This will kick off the training job which should take around 1 hour. There is also the option to use distributed training with more instances, see here:https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html. Running this training with 2 distributed instances should take ~40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e51e544b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.16xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12816\\401471580.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhuggingface_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'datasets'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34mf's3://{bucket}/summarization/data/'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\text_sum\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\text_sum\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[1;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[0;32m   1754\u001b[0m         \"\"\"\n\u001b[0;32m   1755\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1756\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\text_sum\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train request: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[1;32m~\\anaconda3\\envs\\text_sum\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[0;32m    400\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\text_sum\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.16xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'datasets':f's3://{bucket}/summarization/data/'}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad797a-1feb-4432-8e9e-344af30df9d1",
   "metadata": {},
   "source": [
    "### Error trying to run training job above\n",
    "\"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.16xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.\"\n",
    "\n",
    "Pages with info:\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html\n",
    "    - this had \"Depending on your activities and resource usage over time, your SageMaker quotas might be different from the default SageMaker quotas listed on Amazon SageMaker endpoints and quotas in the AWS General Reference. If you encounter error messages that you've exceeded your quota and you need to scale up your SageMaker resources, follow the steps in the Request a service quota increase for SageMaker resources procedure on this page to request a quota increase from AWS Support.\"\n",
    "- I followed the instructions to submit the resource increase request:\n",
    "\n",
    "Limit increase request 1\n",
    "Service: SageMaker Training Jobs\n",
    "Region: US East (Northern Virginia)\n",
    "Resource Type: SageMaker Training\n",
    "Limit name: ml.p3.16xlarge\n",
    "New limit value: 2\n",
    "------------\n",
    "Use case description: I'm learning how to use Sagemaker to train models. I'm following an example written by Heiko Hotz (Senior Solutions Architect at AWS) posted at https://towardsdatascience.com/setting-up-a-text-summarisation-project-introduction-526622eea4a8 . I'm unable to launch the training job due to the following error:\n",
    "\n",
    "ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.16xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.\n",
    "\n",
    "I'm trying to run this job remotely instead of from a Sagemaker Jupyterlab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9c5b8-42b1-4574-9e4d-066172ce9f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
