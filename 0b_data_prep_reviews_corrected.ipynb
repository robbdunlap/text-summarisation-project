{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bac51e",
   "metadata": {},
   "source": [
    "# Part 0 - Data preparation\n",
    "\n",
    "In this notebook we will download the arXiv dataset and save it to S3. We will also do some light data preprocessing by only keeping the columns we need, filtering out reviews that are too short, and limiting the size of the datasets.\n",
    "\n",
    "To read more, please check out https://towardsdatascience.com/setting-up-a-text-summarisation-project-introduction-526622eea4a8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880efb0c",
   "metadata": {},
   "source": [
    "First of all we want to make sure that the relevent libraries are installed on this machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe74630",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74713c30",
   "metadata": {},
   "source": [
    "We will download the dataset directly from the Kaggle website so we need to install the Kaggle Python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Kaggle/kaggle-api\n",
    "# follow the instruction in the above link to export Kaggle username and key to local environment so you don't \n",
    "# have to put it in the notebook (so it won't be exposed on GitHub)\n",
    "\n",
    "# only run the below if you aren't able to export these to your local environment through your shell\n",
    "# import os\n",
    "# os.environ['KAGGLE_USERNAME'] = \"<your-kaggle-username>\"\n",
    "# os.environ['KAGGLE_KEY'] = \"<your-kaggle-api-key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413c346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d7f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.api.dataset_download_files('Cornell-University/arxiv', path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2b814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  arxiv.zip\n",
      "  inflating: arxiv-metadata-oai-snapshot.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip arxiv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd20d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beba5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir raw_data\n",
    "!mv arxiv.zip raw_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8100b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv arxiv-metadata-oai-snapshot.json raw_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc9f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-data_dir=.%2Fraw_data%2F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset arxiv_dataset/default (download: Unknown size, generated: 2.09 GiB, post-processed: Unknown size, total: 2.09 GiB) to /Users/robbdunlap/.cache/huggingface/datasets/arxiv_dataset/default-data_dir=.%2Fraw_data%2F/1.1.0/242eb95c95350194872f5be3fb00e7938e53b0944442e85f45a5d2240328f370...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset arxiv_dataset downloaded and prepared to /Users/robbdunlap/.cache/huggingface/datasets/arxiv_dataset/default-data_dir=.%2Fraw_data%2F/1.1.0/242eb95c95350194872f5be3fb00e7938e53b0944442e85f45a5d2240328f370. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"arxiv_dataset\", data_dir='./raw_data/', split='train', ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605aae99",
   "metadata": {},
   "source": [
    "The original dataset is too long, so we shuffle it and limit the number of records to 25,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6efedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'update_date'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.select(range(25000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c0f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d41258",
   "metadata": {},
   "outputs": [],
   "source": [
    " # only keep columns that are required\n",
    "df = df[['abstract', 'title']]\n",
    "df = df.rename(columns={\"abstract\": \"text\", \"title\": \"summary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45044aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657c2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0149280b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The rest-frame UV spectra of three recent tidal disruption events (TDEs), ASASSN-14li, PTF15af...</td>\n",
       "      <td>Carbon and Nitrogen Abundance Ratio In the Broad Line Region of Tidal   Disruption Events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inspired by the success of transformer-based pre-training methods on natural language tasks an...</td>\n",
       "      <td>Survey: Transformer based Video-Language Pre-training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pandharipande-Pixton have used the geometry of the moduli space of stable quotients to produce...</td>\n",
       "      <td>Tautological relations in moduli spaces of weighted pointed curves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suppose X is a projective toric scheme defined over a commutative ring R equipped with an ampl...</td>\n",
       "      <td>A splitting result for the algebraic K-theory of projective toric   schemes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We introduce an approach that accurately reconstructs 3D human poses and detailed 3D full-body...</td>\n",
       "      <td>Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies   from Single RGB Images</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0    The rest-frame UV spectra of three recent tidal disruption events (TDEs), ASASSN-14li, PTF15af...   \n",
       "1    Inspired by the success of transformer-based pre-training methods on natural language tasks an...   \n",
       "2    Pandharipande-Pixton have used the geometry of the moduli space of stable quotients to produce...   \n",
       "3    Suppose X is a projective toric scheme defined over a commutative ring R equipped with an ampl...   \n",
       "4    We introduce an approach that accurately reconstructs 3D human poses and detailed 3D full-body...   \n",
       "\n",
       "                                                                                          summary  \n",
       "0       Carbon and Nitrogen Abundance Ratio In the Broad Line Region of Tidal   Disruption Events  \n",
       "1                                           Survey: Transformer based Video-Language Pre-training  \n",
       "2                              Tautological relations in moduli spaces of weighted pointed curves  \n",
       "3                     A splitting result for the algebraic K-theory of projective toric   schemes  \n",
       "4  Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies   from Single RGB Images  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c4750",
   "metadata": {},
   "source": [
    "## Filtering the dataset\n",
    "\n",
    "We want to discard reviews and titles that are too short, so that our model can produce more interesting summaries.\n",
    "\n",
    "**How this works**\n",
    "This splits the sentences in both the summary and text columns, selects thoses that are greater than or equal to the minimum cutoff number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53102dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_summary = 5\n",
    "cutoff_text = 20\n",
    "df = df[(df['summary'].apply(lambda x: len(x.split()) >= cutoff_summary)) & (df['text'].apply(lambda x: len(x.split()) >= cutoff_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0b156d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23719"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769ad53",
   "metadata": {},
   "source": [
    "## Limiting the size of the datasets and splitting\n",
    "\n",
    "We want to limit the size of the datasets so that training of the model can finish in a reasonable amount of time. This is a decision that we might want to revisit in the experimentation phase if we want to increase the performance of the model. We then split the dataset into test (80%), validation (10%), and test (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5f42ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(20000, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29b5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# split the dataset into train, val, and test\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=44), [int(0.8*len(df)), int((0.9)*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c36f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data/train.csv', index=False)\n",
    "df_val.to_csv('data/val.csv', index=False)\n",
    "df_test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98223a6c",
   "metadata": {},
   "source": [
    "## Save the data as CSV files and upload them to S3\n",
    "\n",
    "We need to upload the data to S3 in order to train the model at a later point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5cab8",
   "metadata": {},
   "source": [
    "# Connecting to AWS\n",
    "For the below to work you'll need to configure your environment to work with AWS, if you don't you'll get **\"ValueError: Must setup local AWS configuration with a region supported by SageMaker.\"** <br> \n",
    "\n",
    "<u>Steps</u>\n",
    "1. Install AWS Command Line Interface (CLI) on your system (https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)\n",
    "(I used Homebrew to install instead of using curl/sudo).\n",
    "2. Create an AWS Identify and Access Management (IAM) profile (https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html). This is like creating a user account that doesn't have root access on Unix.\n",
    "3. Create access keys for the IAM profile (https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-creds).\n",
    "4. Configure your local environment using the AWS CLI tool (same link as above).\n",
    "\n",
    "The next problem was a bugger to figure out. The instructional text showed go set \"role\" using:\n",
    "```python\n",
    "role = sagemaker.get_execution_role()\n",
    "```\n",
    "but this doesn't work (it only works in a Jupyter notebook within Sagemaker, not when you're executing code on your local maching and trying to work through the AWS API). Instead, do the following to create the \"role\" variable:\n",
    "\n",
    "1. go to the IAM dashboard (https://us-east-1.console.aws.amazon.com/iamv2/home#/users).\n",
    "2. Click on \"Users\" in the left-hand side navigation panel.\n",
    "3. Click on the \"User Name\" you created when creating the IAM profile above.\n",
    "4. Copy the \"User ARN\" that is at the top of the \"Summary\" section on the resulting page\n",
    "5. Set the \"role\" variable equal to this value:\n",
    "```python\n",
    "role = arn:aws:iam::123456789:user/IAMaccountname \n",
    "```\n",
    "\n",
    "If this still doesn't work, verify that your gave the IAM account access to S3 by clicking on the \"Access Advisor\" tab in the \"Summary\" section. If the account has access it will have \"Amazon S3\" listed as a service.\n",
    "\n",
    "After having done these step the below code will run properly.\n",
    "\n",
    "**\\*\\*Do not save your role ARN value to GitHub as it probably can be used by an attacker to get access to your account\\*\\***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c83b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(f\"AWS Region: {region}\")\n",
    "role = 'put your ARN info here - as described above'\n",
    "print(f\"Role Arn: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccac070",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp data/train.csv s3://$bucket/summarization/data/train.csv\n",
    "!aws s3 cp data/val.csv s3://$bucket/summarization/data/val.csv\n",
    "!aws s3 cp data/test.csv s3://$bucket/summarization/data/test.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
